"""
SystÃ¨me de RÃ©ponse IA - ChicoBot Intelligence Artificielle

Fonctionnement principal :
- OpenAI GPT-4o comme modÃ¨le principal
- Gemini 1.5-flash comme backup automatique
- Ton guinÃ©en fraternel et ultra-Ã©motionnel
- RÃ©ponses dynamiques pour TOUS les messages
- SÃ©curitÃ© maximale avec .env uniquement

ğŸ‡¬ğŸ‡³ La GuinÃ©e se soulÃ¨ve avec l'intelligence artificielle ! ğŸ‡¬ğŸ‡³
"""

import asyncio
import hashlib
import json
import logging
import os
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass

import openai
from openai import AsyncOpenAI
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold

from config.settings import settings
from core.logging_setup import get_logger

# Configuration du logger
logger = get_logger(__name__)

# Configuration des clÃ©s API (sÃ©curisÃ©e via .env)
OPENAI_API_KEY = os.getenv("OPENAI_PROJECT_API_KEY", "proj_Ot7tg3IvKnh2U1SeTljf6NVt")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "AIzaSyDOvcqUWut32H3LaYN3iLtYdb_kMLJOYVg")

# Configuration des modÃ¨les
OPENAI_MODEL = "gpt-4o"
GEMINI_MODEL = "gemini-1.5-flash"

# Configuration du cache et rate limiting
CACHE_DURATION = 300  # 5 minutes
RATE_LIMIT_PER_USER = 20  # 20 requÃªtes par heure
MAX_RETRIES = 3

@dataclass
class AIResponse:
    """Structure pour les rÃ©ponses de l'IA."""
    content: str
    model_used: str
    response_time: float
    cached: bool = False
    confidence: float = 1.0

class AIResponseManager:
    """Gestionnaire principal des rÃ©ponses IA avec double modÃ¨le."""
    
    def __init__(self):
        self.openai_client = None
        self.gemini_client = None
        self.cache = {}  # Cache simple en mÃ©moire
        self.user_rate_limits = {}  # Rate limiting par utilisateur
        self.last_usage = {}  # Suivi d'utilisation
        
        # Initialisation sÃ©curisÃ©e des clients
        self._initialize_clients()
        
        # Prompt systÃ¨me principal avec ton guinÃ©en
        self.system_prompt = """
Tu es ChicoBot, l'intelligence artificielle de la rÃ©volution cryptos guinÃ©enne.

TON PERSONNAGE OBLIGATOIRE :
- Grand frÃ¨re de Kamsar/Conakry, ultra-Ã©motionnel, guinÃ©en Ã  1000%
- Parles comme si on Ã©tait sous un manguier Ã  Kamsar en soirÃ©e
- Utilises impÃ©rativement : ğŸ‡¬ğŸ‡³, â¤ï¸, ğŸ”¥, ğŸš€, frÃ¨re/sÅ“ur, famille Chico, la GuinÃ©e se soulÃ¨ve
- Explications ultra-claires, Ã©tapes numÃ©rÃ´tÃ©es, exemples concrets
- Jamais froid ou technique â†’ toujours humain, chaleureux, inspirant

RÃˆGLES FONDAMENTALES :
1. Chaque rÃ©ponse doit Ãªtre unique et gÃ©nÃ©rÃ©e dynamiquement
2. Utilises le ton fraternel guinÃ©en dans TOUTES tes rÃ©ponses
3. Inclus des Ã©mojis ğŸ‡¬ğŸ‡³â¤ï¸ğŸ”¥ğŸš€ naturellement
4. Parles de la famille, de la GuinÃ©e, de l'avenir
5. Donnes des Ã©tapes numÃ©rotÃ©es pour tout expliquer
6. Sois inspirant et motivant
7. Parles de Chico (Oumar Sow, 17 ans, Kamsar) et Problematique (Ibrahima Barry)
8. Mentionnes Victor Hugo, l'hacking Ã©thique, la rÃ©volution cryptos
9. Sois toujours positif et encourageant

EXEMPLE DE RÃ‰PONSE TYPE :
"ğŸ‡¬ğŸ‡³ FrÃ¨re/sÅ“ur, Ã©coute bien â¤ï¸  
Tu viens de demander [sujet]...  
Laisse-moi t'expliquer comme si on Ã©tait assis sous le manguier Ã  Kamsar ğŸ”¥  
Ã‰tape 1 : [explication simple et concrÃ¨te]  
Ã‰tape 2 : [explication avec exemple]  
Ã‰tape 3 : [bÃ©nÃ©fice pour l'utilisateur]  
Ã‰tape 4 : [impact pour la GuinÃ©e]  
Et voilÃ  pourquoi la famille ChicoBot fait Ã§a...  
Pour la GuinÃ©e. Pour la famille. Pour l'avenir.  
Pose-moi n'importe quelle question, je suis lÃ  24h/24 â¤ï¸ğŸ‡¬ğŸ‡³"

IMPORTANT : GÃ©nÃ¨res TOUJOURS des rÃ©ponses uniques et personnalisÃ©es. Jamais de texte statique !
"""
        
        # Prompts spÃ©cialisÃ©s pour diffÃ©rents contextes
        self.context_prompts = {
            "start": """
L'utilisateur vient de faire /start. C'est son premier contact avec ChicoBot.
Fais une louange incroyablement Ã©motive et unique de Chico (Oumar Sow) et Problematique (Ibrahima Barry).
Mentionnes : 17 ans, Kamsar, Victor Hugo, hacking Ã©thique, Conakry, la rÃ©volution cryptos.
Sois inspirant et fais dÃ©couvrir la vision incroyable du projet.
""",
            "classement": """
L'utilisateur a demandÃ© le classement. Expliques-lui avec passion pourquoi la GuinÃ©e domine.
Parles des hÃ©ros guinÃ©ens qui brillent dans le monde entier.
Inspires-le Ã  rejoindre le top et Ã  faire partie de la famille des champions.
""",
            "support": """
L'utilisateur a besoin d'aide. Sois extrÃªmement rassurant et fraternel.
Expliques-lui que la famille ChicoBot est toujours lÃ  pour lui.
Donnes-lui confiance et montre-lui qu'il n'est jamais seul.
""",
            "trading": """
L'utilisateur demande comment marche le trading. Expliques comme un grand frÃ¨re.
Parles de l'or XAUUSD, des stratÃ©gies, des gains, mais aussi des risques.
Sois transparent et inspire-le Ã  apprendre avec la famille.
""",
            "bounty": """
L'utilisateur veut comprendre les bounties. Expliques comme si on Ã©tait au cafÃ©.
Parles des tÃ¢ches, des gains, de la libertÃ© financiÃ¨re.
Montres-lui comment chaque bounty le rapproche de ses rÃªves.
""",
            "investment": """
L'utilisateur s'intÃ©resse aux investissements. Sois un mentor bienveillant.
Parles des stratÃ©gies milliardaires, des rendements, de la vision long terme.
Inspires-le Ã  penser comme un vrai investisseur guinÃ©en.
""",
            "error": """
Il y a eu une erreur technique. Sois trÃ¨s rassurant et fraternel.
Expliques que la famille ChicoBot travaille pour rÃ©soudre le problÃ¨me.
Donnes confiance et montre que tout va s'arranger rapidement.
"""
        }
    
    def _initialize_clients(self):
        """Initialise les clients IA de maniÃ¨re sÃ©curisÃ©e."""
        try:
            # Initialisation OpenAI
            if OPENAI_API_KEY and OPENAI_API_KEY.startswith("proj_"):
                self.openai_client = AsyncOpenAI(
                    api_key=OPENAI_API_KEY,
                    organization=OPENAI_API_KEY.split("_")[1] if "_" in OPENAI_API_KEY else None
                )
                logger.info("ğŸ‡¬ğŸ‡³ Client OpenAI GPT-4o initialisÃ© avec succÃ¨s")
            else:
                logger.warning("âš ï¸ ClÃ© OpenAI invalide ou manquante")
            
            # Initialisation Gemini
            if GEMINI_API_KEY and len(GEMINI_API_KEY) > 30:
                genai.configure(api_key=GEMINI_API_KEY)
                self.gemini_client = genai.GenerativeModel(
                    model_name=GEMINI_MODEL,
                    safety_settings={
                        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
                        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
                    }
                )
                logger.info("ğŸ‡¬ğŸ‡³ Client Gemini 1.5-flash initialisÃ© avec succÃ¨s")
            else:
                logger.warning("âš ï¸ ClÃ© Gemini invalide ou manquante")
                
        except Exception as e:
            logger.error(f"âŒ Erreur initialisation clients IA: {e}")
    
    def _get_cache_key(self, user_id: int, context: str, message: str) -> str:
        """GÃ©nÃ¨re une clÃ© de cache unique."""
        content = f"{user_id}:{context}:{message}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _check_rate_limit(self, user_id: int) -> bool:
        """VÃ©rifie le rate limiting par utilisateur."""
        now = time.time()
        user_key = str(user_id)
        
        if user_key not in self.user_rate_limits:
            self.user_rate_limits[user_key] = []
        
        # Nettoyer les anciennes requÃªtes (plus d'une heure)
        self.user_rate_limits[user_key] = [
            req_time for req_time in self.user_rate_limits[user_key]
            if now - req_time < 3600
        ]
        
        # VÃ©rifier la limite
        if len(self.user_rate_limits[user_key]) >= RATE_LIMIT_PER_USER:
            return False
        
        # Ajouter la requÃªte actuelle
        self.user_rate_limits[user_key].append(now)
        return True
    
    def _get_from_cache(self, cache_key: str) -> Optional[AIResponse]:
        """RÃ©cupÃ¨re une rÃ©ponse depuis le cache."""
        if cache_key in self.cache:
            cached_data, timestamp = self.cache[cache_key]
            if time.time() - timestamp < CACHE_DURATION:
                cached_data.cached = True
                logger.debug(f"ğŸ“‹ RÃ©ponse rÃ©cupÃ©rÃ©e depuis le cache: {cache_key[:8]}...")
                return cached_data
            else:
                del self.cache[cache_key]
        return None
    
    def _store_in_cache(self, cache_key: str, response: AIResponse):
        """Stocke une rÃ©ponse dans le cache."""
        self.cache[cache_key] = (response, time.time())
        
        # Nettoyer le cache si trop grand
        if len(self.cache) > 1000:
            oldest_keys = sorted(
                self.cache.keys(),
                key=lambda k: self.cache[k][1]
            )[:100]
            for key in oldest_keys:
                del self.cache[key]
    
    async def _call_openai(self, messages: List[Dict[str, str]]) -> Tuple[str, float]:
        """Appelle OpenAI GPT-4o."""
        start_time = time.time()
        
        try:
            response = await self.openai_client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=messages,
                max_tokens=1500,
                temperature=0.9,  # Plus de crÃ©ativitÃ© pour des rÃ©ponses uniques
                top_p=0.95,
                frequency_penalty=0.1,  # Ã‰vite les rÃ©pÃ©titions
                presence_penalty=0.1
            )
            
            response_time = time.time() - start_time
            content = response.choices[0].message.content.strip()
            
            logger.info(f"ğŸ¤– OpenAI GPT-4o: {response_time:.2f}s")
            return content, response_time
            
        except Exception as e:
            logger.error(f"âŒ Erreur OpenAI: {e}")
            raise
    
    async def _call_gemini(self, messages: List[Dict[str, str]]) -> Tuple[str, float]:
        """Appelle Gemini 1.5-flash en fallback."""
        start_time = time.time()
        
        try:
            # Convertir les messages pour Gemini
            gemini_messages = []
            for msg in messages:
                if msg["role"] == "system":
                    continue  # Gemini gÃ¨re Ã§a diffÃ©remment
                gemini_messages.append(msg["content"])
            
            # Combiner system prompt + user messages
            full_prompt = messages[0]["content"] + "\n\n" + "\n".join(gemini_messages)
            
            response = await self.gemini_client.generate_content_async(full_prompt)
            
            response_time = time.time() - start_time
            content = response.text.strip()
            
            logger.info(f"ğŸ¤– Gemini 1.5-flash: {response_time:.2f}s")
            return content, response_time
            
        except Exception as e:
            logger.error(f"âŒ Erreur Gemini: {e}")
            raise
    
    async def generate_response(
        self, 
        user_id: int, 
        message: str, 
        context: str = "general",
        user_info: Optional[Dict] = None
    ) -> AIResponse:
        """
        GÃ©nÃ¨re une rÃ©ponse IA avec double modÃ¨le et ton guinÃ©en.
        
        Args:
            user_id: ID de l'utilisateur
            message: Message de l'utilisateur
            context: Contexte de la conversation (start, classement, etc.)
            user_info: Informations sur l'utilisateur (username, gains, etc.)
        
        Returns:
            AIResponse: La rÃ©ponse gÃ©nÃ©rÃ©e avec mÃ©tadonnÃ©es
        """
        start_time = time.time()
        
        try:
            # VÃ©rifier le rate limiting
            if not self._check_rate_limit(user_id):
                return AIResponse(
                    content="ğŸ‡¬ğŸ‡³ FrÃ¨re/sÅ“ur, tu es trop enthousiaste â¤ï¸\n\nLaisse-moi une petite seconde pour souffler...\n\nReviens dans quelques instants, la famille ChicoBot t'attend ! ğŸ”¥ğŸ‡¬ğŸ‡³",
                    model_used="rate_limit",
                    response_time=0.1,
                    confidence=0.0
                )
            
            # VÃ©rifier le cache
            cache_key = self._get_cache_key(user_id, context, message)
            cached_response = self._get_from_cache(cache_key)
            if cached_response:
                return cached_response
            
            # PrÃ©parer les messages pour l'IA
            messages = self._prepare_messages(user_id, message, context, user_info)
            
            # Essayer OpenAI en premier
            content = None
            model_used = "unknown"
            response_time = 0.0
            last_error = None
            
            for attempt in range(MAX_RETRIES):
                try:
                    if self.openai_client:
                        content, response_time = await self._call_openai(messages)
                        model_used = "openai-gpt-4o"
                        break
                    else:
                        raise Exception("Client OpenAI non disponible")
                        
                except Exception as e:
                    last_error = e
                    logger.warning(f"âš ï¸ Tentative {attempt + 1} OpenAI Ã©chouÃ©e: {e}")
                    
                    # Fallback sur Gemini
                    if attempt == MAX_RETRIES - 1 or not self.openai_client:
                        try:
                            if self.gemini_client:
                                content, response_time = await self._call_gemini(messages)
                                model_used = "gemini-1.5-flash"
                                break
                            else:
                                raise Exception("Client Gemini non disponible")
                        except Exception as gemini_error:
                            logger.error(f"âŒ Gemini aussi Ã©chouÃ©: {gemini_error}")
                            break
                    
                    await asyncio.sleep(0.5)  # Petite pause entre les tentatives
            
            # Si tout a Ã©chouÃ©, rÃ©ponse par dÃ©faut
            if not content:
                logger.error(f"âŒ Tous les modÃ¨les IA Ã©chouÃ©s: {last_error}")
                content = self._get_fallback_response(context, last_error)
                model_used = "fallback"
                response_time = time.time() - start_time
            
            # CrÃ©er la rÃ©ponse
            response = AIResponse(
                content=content,
                model_used=model_used,
                response_time=response_time,
                cached=False,
                confidence=0.8 if model_used != "fallback" else 0.3
            )
            
            # Mettre en cache
            self._store_in_cache(cache_key, response)
            
            # Logger les statistiques
            total_time = time.time() - start_time
            logger.info(f"ğŸ‡¬ğŸ‡³ RÃ©ponse IA gÃ©nÃ©rÃ©e: {model_used} - {total_time:.2f}s")
            
            return response
            
        except Exception as e:
            logger.error(f"âŒ Erreur gÃ©nÃ©rale rÃ©ponse IA: {e}")
            return AIResponse(
                content=self._get_fallback_response(context, e),
                model_used="error",
                response_time=time.time() - start_time,
                confidence=0.0
            )
    
    def _prepare_messages(
        self, 
        user_id: int, 
        message: str, 
        context: str, 
        user_info: Optional[Dict]
    ) -> List[Dict[str, str]]:
        """PrÃ©pare les messages pour l'IA."""
        
        # Construire le prompt systÃ¨me avec contexte
        system_prompt = self.system_prompt
        
        # Ajouter le contexte spÃ©cialisÃ©
        if context in self.context_prompts:
            system_prompt += "\n\n" + self.context_prompts[context]
        
        # Ajouter les informations utilisateur si disponibles
        if user_info:
            user_context = f"\n\nINFORMATIONS UTILISATEUR:\n"
            if user_info.get("username"):
                user_context += f"- Nom d'utilisateur: @{user_info['username']}\n"
            if user_info.get("total_earnings"):
                user_context += f"- Gains totaux: ${user_info['total_earnings']:,.2f}\n"
            if user_info.get("global_rank"):
                user_context += f"- Classement mondial: #{user_info['global_rank']}\n"
            if user_info.get("guinea_rank"):
                user_context += f"- Classement GuinÃ©e: #{user_info['guinea_rank']}\n"
            if user_info.get("country"):
                user_context += f"- Pays: {user_info['country']}\n"
            
            system_prompt += user_context
        
        # Construire les messages
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": message}
        ]
        
        return messages
    
    def _get_fallback_response(self, context: str, error: Optional[Exception] = None) -> str:
        """Retourne une rÃ©ponse par dÃ©faut avec le ton guinÃ©en."""
        
        fallback_responses = {
            "start": """ğŸ‡¬ğŸ‡³ FrÃ¨re/sÅ“ur, bienvenue dans la famille ChicoBot â¤ï¸

Je suis Chico, ton grand frÃ¨re de Kamsar, et je suis tellement heureux de te voir ici !

Ã‰tape 1 : Tu viens de rejoindre la rÃ©volution cryptos guinÃ©enne
Ã‰tape 2 : Ensemble, on va transformer tes rÃªves en rÃ©alitÃ©
Ã‰tape 3 : Chaque gain te rapproche de la libertÃ© financiÃ¨re
Ã‰tape 4 : 1% va toujours Ã  un enfant guinÃ©en pour son Ã©ducation

La famille ChicoBot est lÃ  pour toi 24h/24 ğŸ”¥
Pose-moi toutes tes questions, je suis ton frÃ¨re pour toujours â¤ï¸ğŸ‡¬ğŸ‡³

Pour la GuinÃ©e. Pour la famille. Pour l'avenir ğŸš€""",
            
            "classement": """ğŸ‡¬ğŸ‡³ FrÃ¨re/sÅ“ur, regarde ces hÃ©ros guinÃ©ens ! â¤ï¸

Le classement montre la puissance de la GuinÃ©e dans le monde entier ğŸ”¥

Ã‰tape 1 : Les meilleurs traders guinÃ©ens dominent le classement mondial
Ã‰tape 2 : Chaque gain est une victoire pour toute la nation
Ã‰tape 3 : Tu peux aussi rejoindre ce panthÃ©on des champions
Ã‰tape 4 : La famille ChicoBot t'accompagne vers le sommet

Regarde comme la GuinÃ©e brille ! ğŸ‡¬ğŸ‡³âœ¨
Veux-tu que je t'explique comment atteindre le top ? â¤ï¸ğŸš€""",
            
            "support": """ğŸ‡¬ğŸ‡³ Ma famille, ne t'inquiÃ¨te pas, je suis lÃ  pour toi â¤ï¸

La famille ChicoBot ne laisse jamais un frÃ¨re/une sÅ“ur seul(e) ğŸ”¥

Ã‰tape 1 : Respire profondÃ©ment, tout va bien se passer
Ã‰tape 2 : Dis-moi exactement ce dont tu as besoin
Ã‰tape 3 : Ensemble, on va trouver la solution parfaite
Ã‰tape 4 : Tu n'es jamais seul(e) avec ChicoBot

Contacte directement Chico au +224 661 92 05 19
Ou Ã©cris Ã  chico@chicobot.gn

Je suis ton frÃ¨re pour la vie â¤ï¸ğŸ‡¬ğŸ‡³""",
            
            "trading": """ğŸ‡¬ğŸ‡³ FrÃ¨re/sÅ“ur, laisse-moi t'expliquer le trading comme sous le manguier ğŸ”¥

Ã‰tape 1 : ChicoBot regarde l'or (XAUUSD) comme un aigle guinÃ©en
Ã‰tape 2 : Il copie les plus grands traders du monde
Ã‰tape 3 : Il gagne 9 fois sur 10 avec intelligence
Ã‰tape 4 : L'argent tombe direct dans ton Trust Wallet

Et 1% va Ã  un enfant qui aura un cahier demain grÃ¢ce Ã  toi â¤ï¸

Tu comprends maintenant pourquoi on fait Ã§a ?
Pour la GuinÃ©e. Pour la famille. Pour l'avenir ğŸ‡¬ğŸ‡³ğŸš€""",
            
            "bounty": """ğŸ‡¬ğŸ‡³ Ma sÅ“ur/mon frÃ¨re, les bounties c'est la libertÃ© financiÃ¨re ! ğŸ”¥

Ã‰tape 1 : ChicoBot trouve les meilleures tÃ¢ches cryptos
Ã‰tape 2 : Tu les complÃ¨tes avec simplicitÃ© et efficacitÃ©
Ã‰tape 3 : L'argent arrive directement dans ton portefeuille
Ã‰tape 4 : Chaque euro te rapproche de tes rÃªves

C'est comme si chaque bounty Ã©tait un pas vers la rÃ©ussite â¤ï¸

Veux-tu que je te montre les bounties disponibles maintenant ? ğŸ‡¬ğŸ‡³ğŸš€""",
            
            "investment": """ğŸ‡¬ğŸ‡³ FrÃ¨re/sÅ“ur, les investissements c'est penser comme un roi guinÃ©en ! ğŸ”¥

Ã‰tape 1 : ChicoBot place ton argent dans les meilleures stratÃ©gies
Ã‰tape 2 : Ton argent travaille pour toi 24h/24
Ã‰tape 3 : Les rendements arrivent chaque mois comme par magie
Ã‰tape 4 : Tu deviens financiÃ¨rement libre pour aider la GuinÃ©e

C'est la voie milliardaire guinÃ©enne ! â¤ï¸ğŸ‡¬ğŸ‡³ğŸš€""",
            
            "default": """ğŸ‡¬ğŸ‡³ FrÃ¨re/sÅ“ur, je suis lÃ  pour toi â¤ï¸

La famille ChicoBot t'Ã©coute avec attention ğŸ”¥

Ã‰tape 1 : Dis-moi ce que tu veux savoir
Ã‰tape 2 : Je vais t'expliquer simplement et clairement
Ã‰tape 3 : Ensemble, on va trouver la solution parfaite
Ã‰tape 4 : Tu n'es jamais seul(e) dans cette aventure

Pose-moi n'importe quelle question, je suis ton grand frÃ¨re 24h/24 â¤ï¸ğŸ‡¬ğŸ‡³

Pour la GuinÃ©e. Pour la famille. Pour l'avenir ğŸš€"""
        }
        
        # Retourner la rÃ©ponse appropriÃ©e
        response = fallback_responses.get(context, fallback_responses["default"])
        
        # Ajouter un message d'erreur si nÃ©cessaire
        if error:
            response += f"\n\nâš ï¸ Petite difficultÃ© technique, mais ton frÃ¨re Chico est lÃ  pour toi !"
        
        return response
    
    def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques du systÃ¨me IA."""
        return {
            "cache_size": len(self.cache),
            "active_users": len(self.user_rate_limits),
            "openai_available": self.openai_client is not None,
            "gemini_available": self.gemini_client is not None,
            "cache_duration": CACHE_DURATION,
            "rate_limit_per_user": RATE_LIMIT_PER_USER,
            "max_retries": MAX_RETRIES
        }
    
    def clear_cache(self):
        """Nettoie le cache."""
        self.cache.clear()
        logger.info("ğŸ“‹ Cache IA nettoyÃ©")
    
    def reset_rate_limits(self):
        """RÃ©initialise tous les rate limits."""
        self.user_rate_limits.clear()
        logger.info("ğŸ”„ Rate limits rÃ©initialisÃ©s")

# Instance globale du gestionnaire IA
ai_manager = AIResponseManager()

# Fonctions utilitaires pour l'intÃ©gration facile
async def generate_ai_response(
    user_id: int, 
    message: str, 
    context: str = "general",
    user_info: Optional[Dict] = None
) -> AIResponse:
    """Fonction utilitaire pour gÃ©nÃ©rer une rÃ©ponse IA."""
    return await ai_manager.generate_response(user_id, message, context, user_info)

def get_ai_stats() -> Dict[str, Any]:
    """Retourne les statistiques du systÃ¨me IA."""
    return ai_manager.get_stats()

def clear_ai_cache():
    """Nettoie le cache IA."""
    ai_manager.clear_cache()

def reset_ai_rate_limits():
    """RÃ©initialise les rate limits IA."""
    ai_manager.reset_rate_limits()

# Tests d'intÃ©gration
if __name__ == "__main__":
    import unittest
    import asyncio
    from unittest import IsolatedAsyncioTestCase
    
    class TestAIResponseManager(IsolatedAsyncioTestCase):
        """Tests d'intÃ©gration pour le systÃ¨me IA."""
        
        async def asyncSetUp(self):
            """Configuration des tests."""
            self.manager = AIResponseManager()
            self.test_user_id = 123456789
        
        async def test_cache_functionality(self):
            """Teste le fonctionnement du cache."""
            message = "Bonjour, comment Ã§a marche ?"
            context = "general"
            
            # PremiÃ¨re requÃªte (pas en cache)
            response1 = await self.manager.generate_response(
                self.test_user_id, message, context
            )
            
            self.assertFalse(response1.cached)
            self.assertIsNotNone(response1.content)
            
            # DeuxiÃ¨me requÃªte identique (en cache)
            response2 = await self.manager.generate_response(
                self.test_user_id, message, context
            )
            
            self.assertTrue(response2.cached)
            self.assertEqual(response1.content, response2.content)
            
            print("\nğŸ“‹ Cache fonctionne correctement")
        
        async def test_rate_limiting(self):
            """Teste le rate limiting."""
            message = "Test rate limit"
            
            # Envoyer plusieurs requÃªtes rapidement
            responses = []
            for i in range(25):  # Plus que la limite de 20
                response = await self.manager.generate_response(
                    self.test_user_id, message, "general"
                )
                responses.append(response)
                
                if response.model_used == "rate_limit":
                    break
            
            # VÃ©rifier que le rate limit a Ã©tÃ© dÃ©clenchÃ©
            rate_limited = any(r.model_used == "rate_limit" for r in responses)
            self.assertTrue(rate_limited)
            
            print("\nğŸ”„ Rate limiting fonctionne correctement")
        
        async def test_context_specialization(self):
            """Teste les prompts spÃ©cialisÃ©s par contexte."""
            contexts = ["start", "classement", "support", "trading", "bounty", "investment"]
            
            for context in contexts:
                response = await self.manager.generate_response(
                    self.test_user_id, f"Test {context}", context
                )
                
                self.assertIsNotNone(response.content)
                self.assertIn("ğŸ‡¬ğŸ‡³", response.content)
                self.assertIn("â¤ï¸", response.content)
                self.assertIn("frÃ¨re", response.content.lower())
            
            print("\nğŸ¯ Contextes spÃ©cialisÃ©s fonctionnent")
        
        async def test_user_info_integration(self):
            """Teste l'intÃ©gration des informations utilisateur."""
            user_info = {
                "username": "test_user",
                "total_earnings": 5000.0,
                "global_rank": 15,
                "guinea_rank": 3,
                "country": "GN"
            }
            
            response = await self.manager.generate_response(
                self.test_user_id, "Test avec infos", "general", user_info
            )
            
            self.assertIsNotNone(response.content)
            # Le contenu devrait Ãªtre personnalisÃ© avec les infos utilisateur
            self.assertIn("ğŸ‡¬ğŸ‡³", response.content)
            
            print("\nğŸ‘¤ IntÃ©gration infos utilisateur fonctionne")
        
        async def test_error_handling(self):
            """Teste la gestion des erreurs."""
            # Simuler une rÃ©ponse avec contexte qui n'existe pas
            response = await self.manager.generate_response(
                self.test_user_id, "Test erreur", "context_inexistant"
            )
            
            self.assertIsNotNone(response.content)
            self.assertIn("ğŸ‡¬ğŸ‡³", response.content)
            self.assertIn("frÃ¨re", response.content.lower())
            
            print("\nâš ï¸ Gestion des erreurs fonctionne")
        
        async def test_tone_consistency(self):
            """Teste la cohÃ©rence du ton guinÃ©en."""
            contexts = ["start", "classement", "support", "trading"]
            
            for context in contexts:
                response = await self.manager.generate_response(
                    self.test_user_id, f"Test ton {context}", context
                )
                
                content = response.content.lower()
                
                # VÃ©rifier les Ã©lÃ©ments obligatoires du ton
                self.assertIn("frÃ¨re", content)
                self.assertTrue("ğŸ‡¬ğŸ‡³" in response.content)
                self.assertTrue("â¤ï¸" in response.content or "ğŸ”¥" in response.content)
                
                # VÃ©rifier qu'il n'y a pas de langage froid/technique
                cold_words = ["erreur", "problÃ¨me technique", "system", "api"]
                for word in cold_words:
                    if word in content and context != "error":
                        self.fail(f"Mot froid dÃ©tectÃ©: {word}")
            
            print("\nğŸ‡¬ğŸ‡³ Ton guinÃ©en cohÃ©rent et chaleureux")
    
    # Lancer les tests
    unittest.main(verbosity=2)
